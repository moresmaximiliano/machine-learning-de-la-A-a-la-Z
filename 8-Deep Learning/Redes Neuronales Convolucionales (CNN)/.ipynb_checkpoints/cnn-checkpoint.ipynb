{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KdUFcDsdzRyw"
   },
   "source": [
    "# Clonamos el repositorio para obtener los dataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "mHReFf3_y9ms",
    "outputId": "c17545fd-c7dd-42c2-e3ad-4f55db21611f"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/joanby/machinelearning-az.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vNKZXgtKzU2x"
   },
   "source": [
    "# Damos acceso a nuestro Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "5gu7KWnzzUQ0",
    "outputId": "abe602b4-3a59-470e-d508-037c6966002b"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1gUxIkHWzfHV"
   },
   "source": [
    "# Test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mIQt3jBMzYRE"
   },
   "outputs": [],
   "source": [
    "!ls '/content/drive/My Drive' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mHsK36uN0XB-"
   },
   "source": [
    "# Google colab tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kTzwfUPWzrm4"
   },
   "outputs": [],
   "source": [
    "from google.colab import files # Para manejar los archivos y, por ejemplo, exportar a su navegador\n",
    "import glob # Para manejar los archivos y, por ejemplo, exportar a su navegador\n",
    "from google.colab import drive # Montar tu Google drive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uab9OAbV8hYN"
   },
   "source": [
    "# Instalar dependendias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "qukjDgj98kE4",
    "outputId": "95b5f2b5-7149-436a-b1fb-ad567cc783bd"
   },
   "outputs": [],
   "source": [
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instalar Theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade --no-deps git+git://github.com/Theano/Theano.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instalar Tensorflow y Keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install keras\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3yFpBwmNz70v"
   },
   "source": [
    "# Redes Neuronales Convolucionales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v8OxSXXSz-OP"
   },
   "source": [
    "# Cómo importar las librerías\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "edZX51YLzs59"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# En este caso, tanto las variables dependientes como las independientes no estan representadas por columnas de un csv. Las entradas de la ANN (VI) estarán representadas por los píxeles conseguidos luego de aplanar lo procesado por las capas de la CNN\n",
    "# Tendremos imágenes en carpetas listas para ser usadas como train/test de perros y gatos, ya etiquetadas\n",
    "# Si queremos reconocer objetos, sea el problema que sea, debemos agregar una carpeta con un número elevado de fotos de ese objeto, así con cada objeto que querramos reconocer\n",
    "# Intentar que las fotos que se usen para entrenar el reconocimiento de cada objeto sean de la misma cantidad. Esto evitará que la CNN tienda a predecir sólo el objeto con más imágenes cargadas\n",
    "# Nosotros tenemos 4000/1000 de perros y gatos para entrenar/testear, dandonos un total de 10000 imágenes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 1 - Construir el modelo de CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# De preprocesado como tal, no debemos hacer nada, ya que las librerias que luego vamos a usar nos permiten configurar la entrada de píxeles aplanados a la CNN\n",
    "# Ver los pasos para construit CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SsVEdPzf4XmV"
   },
   "source": [
    "# Importar las librerías y paquetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v9CtwK834bjy"
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential # para crear/inicializar NN secuencial junto con los pesos aleatorios\n",
    "from keras.layers import Conv2D # para convolución, que filtrará las imáganes creando mapas de características (3D para tratar con videos o imágenes volumétricas)\n",
    "from keras.layers import MaxPooling2D # para pooling, que disminuye las dimensiones de los mapas de características y resume la información que contiene cada uno de ellos(3D para tratar con videos o imágenes volumétricas)\n",
    "from keras.layers import Flatten # para aplanar las matrices y convertirlas en un vector que se utiliza como entrada de la ANN\n",
    "from keras.layers import Dense # para añadir una capa de ANN tradicional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inicializar la CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paso 1 - Convolución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Convolución: Aplicamos a la imagen filtros para obtener de ella mapas de caracteristicas (detectores de rasgos)\n",
    "classifier.add(Conv2D(filters = 32,kernel_size = (3, 3), \n",
    "                      input_shape = (64, 64, 3), activation = \"relu\")) # Usamos Conv2D para añadir una capa de convolución\n",
    "# filters: cantidad de mapas de características a obtener (detectores de rasgos/filtros a aplicar a la imagen) (podemos probar con 64, 128 o cualquier otra potencia de 2, pero primero asegurarnos de que nuestro PC aguanta y si podemos obtener buenos resultados), kernel_size: tamaño de los detectores de rasgos/filtros en píxeles (resta en n-1 píxeles del filtro la dimensión de la imagen, usamos 3x3 ya que las imágenes de entrada no son tan grandes), strides y padding: se usan para delimitar si los filtros se aplicaran sobre el pixel central o no (generalmente se deja, aplicando el filtro sobre el píxel central)\n",
    "# input_shape: en este caso aclaramos los tamaños de píxeles de las imágenes de entrada (filas, columnas) y sus canales (1 si es blanco y negro, 3 si es a color). Intentar entrenar la CNN con imáganes pequeñas para no pasarse con coste computacional, y decidir si es importante o no mantener el color de la imágen para identificar objetos (ya que con escala de grises ahorramos más recursos), activation: función de activación a utilizar en la capa de convolución, generalmente usaremos ReLu para eliminar la linealidad en la identificación de rasgos, los cuales generalmente no son lineales\n",
    "# Si las imágenes son mayores a lo que se busca en entrada podemos aplicar interpolación, lo que ajusta la imagen a las dimensiones requeridas sin perder la proporción. Alternativamente, también se puede aplicar recorte (cropping) para seleccionar una porción central o específica de la imagen. Si las imágenes son menores, podemos aplicar  interpolación (escalado hacia arriba), lo cual puede llevar a una pérdida de calidad o pixelación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paso 2 - Max Pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max Pooling: Resumimos la información extraida por los filtros (mapas de características) en imáganes de menores dimensiones según el promedio o valor máximo de píxeles por ventana (pool_size). \n",
    "classifier.add(MaxPooling2D(pool_size = (2,2))) # pool_size: tamaño de ventana que se le aplica, si lo hacemos de 2x2, balance entre pérdida de información y reducir costos computacionales (con 3x3 y 4x4 se reduce aún más el tamaño de la imagen (pérdida de información con imáganes pequeñas), sirviéndonos para procesar imágenes con mayor resolución), nos quedan los mapas de la mitad de filas y columnas que en el paso anterior (+1 para impares)\n",
    "# strides: númnero de enteros que tiene que correr de un lado a otro, podemos usar para no superponer el análisis de los mismos píxeles, si no se especifica se ajusta al tamaño de ventana, saltenado el análisis de píxeles de manera que nunca se posicione 2 veces sobre el mismo píxel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Una segunda capa de convolución y max pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(Conv2D(filters = 32,kernel_size = (3, 3), activation = \"relu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(MaxPooling2D(pool_size = (2,2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paso 3 - Flattening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flattening: Transformo los mapas de características (matriz de píxeles) ya procesados por las operaciones de convolución y pooling en un gran vector único, para usarlo como entrada de la ANN (Capa Full-Conect). Aplicamos las operaciones de Convolución y Pooling para que la ANN no tenga miles de entradas para analizar, resumiendo/simplificando la información de la imagen en los mapas\n",
    "classifier.add(Flatten()) # genera el vector de entrada de la red neuronal\n",
    "# Si aplanaramos directamente la imágen (sin aplicar convolución y pooling) no tendríamos tampoco reconocido en el vector las características/rasgos distintivos de la imagen, por lo que el algoritmo no aprendería a reconocer rasgos comunes de objetos, sino que se limitaría a identificar un mismo rango de color de pixeles aplicable para reconocer un mismo tipo de objeto. Osea que con esas operaciones también extraería más información de la imagen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paso 4 - Full Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilizo una ANN para procesar los datos del vector aplanado\n",
    "classifier.add(Dense(units = 128, activation = \"relu\")) # capa oculta con activación relu, para ver si una neurona se activa o no en consideración con la información que brinda para predecir (recomendación: promedio de número de nodos de entrada y de capa de salida (potencia de 2 si es muy grande), pensar que el vector posee muchos valores)\n",
    "classifier.add(Dense(units = 1, activation = \"sigmoid\")) # capa de salida con activación sigmoid, para que nos brinde probabilidades de pertenencia a las clases (clasificaciónn binaria, por eso también un nodo o valor de salida; si fuera múltiple deberíamos aplicar Softmax y Entropía Cruzada y poner tantas neuronas como categorías tenga)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compilar la CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.compile(optimizer = \"adam\", loss = \"binary_crossentropy\", metrics = [\"accuracy\"]) # arrancamos la CNN, e indicarle la función de costes y el proceso de propagación hacia atras\n",
    "# optimizer: GD, GDE, adam; loss: Error Cuadrado Medio, etc, pero en problemas de clasificación solemos usar clasificadores relacionadas con la Reg Logística (entropía cruzada, en este caso binaria); metrics: métrica de precisión (podemos agregar todas las que querramos en los corchetes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parte 2 - Ajustar la CNN a las imágenes para entrenar (perros y gatos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator # para importar/cargar/preprocesar imágenes cuando tratemos con CNN, evitando el sobreajuste sobre los datos de train (espera que ya separemos en carpetas las imagenes usadas para train y para test)\n",
    "# Necesitaremos muchas imágenes para entrenar a la red y luego usaremos la técnica de limpieza 'aumento de imágen': iremos seleccionando aleatoriamente algunas de ellas para hacerle modificaciones/transformaciones, evitando caer en el sobreajuste de la red sobre los datos de entrenamiento\n",
    "# El aplicado de esta técnica nos 'enriquece' o 'aumenta artificialmente' el tamaño del dataset, pasando de usar en el entrenamiento 8000 imágenes a casi 100000 de las mismas (que no se almacenan, sino que se generan cuando entrenamos al modelo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocesado para las imágenes del conjunto de entrenamiento (reescalamos valores de píxeles y le aplicamos variaciones a las imágenes para que las reconozca con modificaciones incluidas)\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255, # reescalado de los píxeles (pasamos de 0-255 a 0-1, normalizando y permitiendo a la ANN ser más eficaz)\n",
    "        shear_range=0.2, # rango de modificación (imágenes serán distorsionadas aleatoriamente a lo largo de una dirección, desplazando parte del contenido de la imagen en esa dirección). Podemos aumentar los rangos para preparar el reconocimiento de la red ante cambios más bruscos en las entradas\n",
    "        zoom_range=0.2, # rango de zoom (aplica un zoom aleatorio a las imágenes dentro del rango especificado. En este caso, el rango es 0.2, lo que significa que el zoom puede variar hasta un 20% del tamaño original de la imagen)\n",
    "        horizontal_flip=True) # si permitimos flips horizontales\n",
    "\n",
    "# Preprocesado para las imáganes del conjunto de entrenamiento (solo le aplicamos reescalado de píxeles)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255) # (pasamos de 0-255 a 0-1, normalizando y permitiendo a la ANN ser más eficaz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# En nuestro problema particular, vamos a intentar entrenar una CNN para que sepa diferenciar entre imágenes de perros y gatos. Pero podemos reemplazar los set de imágenes por otras que querramos usar para entrenar/identificar objetos.\n",
    "training_dataset = train_datagen.flow_from_directory('dataset/training_set', # set de train\n",
    "                                                    target_size=(64, 64), # tamaño en el que cargo las imáganes (mantener igual que en CNN)\n",
    "                                                    batch_size=32, # cantidad de imágenes que serán procesadas por la CNN antes de actualizar pesos \n",
    "                                                    class_mode='binary') # método de clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testing_dataset = test_datagen.flow_from_directory('dataset/test_set', # set de test\n",
    "                                                target_size=(64, 64), # tamaño en el que cargo las imágenes (mantener igual que en CNN)\n",
    "                                                batch_size=32, # cantidad de imágenes que serán procesadas por la CNN antes de actualizar pesos\n",
    "                                                class_mode='binary') # método de clasificación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elaborar una matriz de confusión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.fit_generator(training_dataset,\n",
    "                        steps_per_epoch=8000,\n",
    "                        epochs=25,\n",
    "                        validation_data=testing_dataset,\n",
    "                        validation_steps=2000)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "multiple_linear_regression_new_version.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
